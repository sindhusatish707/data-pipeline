# Data Pipeline (ETL/ELT)

## ğŸ¯ Goal

This project is a hands-on implementation of a **scalable data pipeline** inspired by how companies like Robinhood handle large volumes of data.  
The goal is to build end-to-end **ETL workflows** that:

- **Extract** data from external APIs (e.g., stock market data)
- **Transform** and clean the data
- **Load** it into a data lake or warehouse for analysis
- Orchestrate workflows using **Apache Airflow** for scheduling and monitoring

This project serves as a portfolio-ready showcase of **data engineering skills**, demonstrating the ability to build production-grade pipelines using industry-standard tools.

---

## ğŸ› ï¸ Tech Stack

- **Apache Airflow (2.10.2)** â€“ DAG scheduling & orchestration
- **Docker & Docker Compose** â€“ Containerized, reproducible setup
- **PostgreSQL** â€“ Airflow metadata database
- **Celery + Redis** â€“ Distributed task execution
- **Pandas + yfinance** â€“ Data extraction & transformation
- **Python 3.10+** â€“ DAG scripting & pipeline logic

---

## ğŸ“‚ Project Structure

```
Data-Pipeline/
â”œâ”€ airflow-docker/
â”‚ â”œâ”€ dags/ # DAGs (e.g., fetch_stock.py)
â”‚ â”œâ”€ logs/ # Task logs
â”‚ â”œâ”€ plugins/ # Custom operators/hooks (future)
â”‚ â””â”€ config/airflow.cfg # Airflow configuration
â”œâ”€ data/ # Output data (mounted from container)
â”œâ”€ requirements.txt # Python dependencies
â”œâ”€ docker-compose.yaml # Container orchestration
â””â”€ Dockerfile # Custom Airflow image
```

---

## ğŸš€ Current Progress

- âœ… **Set up Airflow with Docker** using CeleryExecutor
- âœ… **Configured DAG folder, logs, and plugins**
- âœ… **Created a sample DAG (`fetch_stock_example`)** that fetches AAPL stock prices from Yahoo Finance and stores them as CSV
- âœ… **Mounted local `data/` folder** for easy inspection of output
- â³ Next: Add data transformations and load step (Postgres/S3/Parquet)

---

## ğŸ§­ Roadmap

- [ ] Add support for multiple symbols (parameterized DAGs)
- [ ] Transform data (add moving averages, daily returns)
- [ ] Load data into Postgres or a data lake
- [ ] Add Airflow sensors & alerts
- [ ] Deploy pipeline to cloud (optional)

---

## ğŸ–¥ï¸ Getting Started

1. **Clone Repo & Install Docker**
2. **Build & Start Airflow**
   ```bash
   docker compose build
   docker compose up -d
   ```
3. **Open Airflow UI**

- Navigate to http://localhost:8080
- Enable and trigger fetch_stock_example

4. **Check Output**

- Look in ./data/AAPL.csv for results
