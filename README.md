# Data Pipeline (ETL/ELT)

## ğŸ¯ Goal

This project is a hands-on implementation of a **scalable data pipeline** inspired by how companies like Robinhood handle large volumes of data.  
The goal is to build end-to-end **ETL workflows** that:

- **Extract** data from external APIs (e.g., stock market data)
- **Transform** and clean the data
- **Load** it into a data lake or warehouse for analysis
- Orchestrate workflows using **Apache Airflow** for scheduling and monitoring

This project serves as a portfolio-ready showcase of **data engineering skills**, demonstrating the ability to build production-grade pipelines using industry-standard tools.

---

## ğŸ› ï¸ Tech Stack

- **Apache Airflow (2.10.2)** â€“ DAG scheduling & orchestration
- **Docker & Docker Compose** â€“ Containerized, reproducible setup
- **PostgreSQL** â€“ Airflow metadata database
- **Celery + Redis** â€“ Distributed task execution
- **Pandas + yfinance** â€“ Data extraction & transformation
- **Python 3.10+** â€“ DAG scripting & pipeline logic

---

## ğŸ“‚ Project Structure

```
Data-Pipeline/
â”œâ”€ airflow-docker/
â”‚ â”œâ”€ dags/ # DAGs (e.g., fetch_stock.py)
â”‚ â”œâ”€ logs/ # Task logs
â”‚ â”œâ”€ plugins/ # Custom operators/hooks (future)
â”‚ â””â”€ config/airflow.cfg # Airflow configuration
â”œâ”€ data/ # Output data (mounted from container)
â”œâ”€ requirements.txt # Python dependencies
â”œâ”€ docker-compose.yaml # Container orchestration
â””â”€ Dockerfile # Custom Airflow image
```

---

## ğŸš€ Current Progress

- âœ… **Set up Airflow with Docker** using CeleryExecutor
- âœ… **Configured DAG folder, logs, and plugins**
- âœ… **Created a sample DAG (`fetch_stock_example`)** that fetches AAPL stock prices from Yahoo Finance and stores them as CSV
- âœ… **Mounted local `data/` folder** for easy inspection of output
- â³ Next: Add data transformations and load step (Postgres/S3/Parquet)

---

## ğŸ§­ Roadmap

- [ ] Add support for multiple symbols (parameterized DAGs)
- [ ] Transform data (add moving averages, daily returns)
- [ ] Load data into Postgres or a data lake
- [ ] Add Airflow sensors & alerts
- [ ] Deploy pipeline to cloud (optional)

---

## ğŸ–¥ï¸ Getting Started

1. **Clone Repo & Install Docker**
2. **Build & Start Airflow**
   ```bash
   docker compose build
   docker compose up -d
   ```
3. **Open Airflow UI**

- Navigate to http://localhost:8080
- Enable and trigger fetch_stock_example

4. **Check Output**

- Look in ./data/AAPL.csv for results

---

## ğŸ“Š Project Architecture

![Pipeline Architecture](docs/architecture.jpg)

The diagram above shows how data flows from the source API to the final warehouse.

### ğŸ§  How the Architecture Works

1. **Extract:** The pipeline calls the Yahoo Finance API to pull raw stock price data.
2. **Raw Zone:** The unprocessed data is saved as CSV files in `data/raw/`. This acts as a â€œdata lakeâ€ where nothing is changed, so we always have the original source.
3. **Transform:** A PySpark job will clean the data, remove duplicates, and calculate new fields (daily return, moving averages). Clean data is saved into `data/processed/`.
4. **Load:** The processed data is written to a PostgreSQL table (or Parquet files in `data/warehouse/`) so analysts and dashboards can query it efficiently.
5. **Orchestration:** Airflow schedules, monitors, and retries each step automatically.
6. **Consumption:** Analysts, dashboards, or ML models can now use the clean data confidently.

## ğŸ—„ï¸ Data Model

![Stock Table Schema](docs/stock_table_schema.jpg)

The schema defines consistent column names and data types for stock data.
